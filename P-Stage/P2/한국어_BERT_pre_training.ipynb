{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "한국어_BERT_pre_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sLaWNa64xAz"
      },
      "source": [
        "# 한국어 BERT 모델 학습\n",
        "\n",
        "> 작성자      \n",
        "```\n",
        "* 김성현 (bananaband657@gmail.com)  \n",
        "김바다 (qkek983@gmail.com)\n",
        "박상희 (parksanghee0103@gmail.com)  \n",
        "이정우 (jungwoo.l2.rs@gmail.com)\n",
        "```\n",
        "[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItbY5oL-V5DW"
      },
      "source": [
        "## 학습 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYqha-sZ4KAS"
      },
      "source": [
        "!mkdir my_data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYitBFxO0uP3"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ7EGqFiJ9EH"
      },
      "source": [
        "실습을 위해서 아주 작은 wiki 코퍼스를 가져와보겠습니다 :-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z0pedAT44Gl",
        "outputId": "a8d508aa-1229-4c17-84a3-14a2dcfeaebb"
      },
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    401      0 --:--:--  0:00:01 --:--:--   401\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 1323k  100 1323k    0     0   983k      0  0:00:01  0:00:01 --:--:--  983k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYXccFQeKB7F"
      },
      "source": [
        "물론 한국어 wiki 전체 코퍼스도 공유드릴게요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq_xSrYBKFuv",
        "outputId": "043f8728-8720-4ab7-914c-85b617e67172"
      },
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" -o my_data/wiki_20190620.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   408    0   408    0     0   3743      0 --:--:-- --:--:-- --:--:--  3743\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  476M    0  476M    0     0   103M      0 --:--:--  0:00:04 --:--:--  113M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI3vg0Q2KW6K"
      },
      "source": [
        "이제 본격적으로 BERT 학습을 위한 준비를 해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id8FcYRa48Gc",
        "outputId": "2850b899-7707-462a-b090-6ea9b3114f1f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 47.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIEUFdT4WIqd"
      },
      "source": [
        "## Tokenizer 만들기\n",
        "\n",
        "corpus를 이용하 wordPiece tokenizer를 만들어보겠습니다.   \n",
        "이제는 익숙하시죠? :-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFTgG7yv4_n7"
      },
      "source": [
        "!mkdir wordPieceTokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGFiNRpV5Ban",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7fd6aa-3bf5-4d64-ccb5-e4f2026df77c"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# Initialize an empty tokenizer\n",
        "wp_tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,   # [\"이순신\", \"##은\", \" \", \"조선\"] ->  [\"이순신\", \"##은\", \"조선\"]\n",
        "    # if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    handle_chinese_chars=True,  # 한자는 모두 char 단위로 쪼게버립니다.\n",
        "    strip_accents=False,    # True: [YehHamza] -> [Yep, Hamza]\n",
        "    lowercase=False,    # Hello -> hello\n",
        ")\n",
        "\n",
        "# And then train\n",
        "wp_tokenizer.train(\n",
        "    files=\"my_data/wiki_20190620_small.txt\",\n",
        "    vocab_size=20000,   # vocab size 를 지정해줄 수 있습니다.\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "    wordpieces_prefix=\"##\"\n",
        ")\n",
        "\n",
        "# Save the files\n",
        "wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wordPieceTokenizer/my_tokenizer-vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngd6Z3aJ57VR",
        "outputId": "eda8a17a-ebca-4cee-8257-ade158a89152"
      },
      "source": [
        "print(wp_tokenizer.get_vocab_size())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnPe0_4a5C-a",
        "outputId": "c747c47a-df35-4802-f284-afe5284e8c2a"
      },
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "tokenized_text = wp_tokenizer.encode(text)\n",
        "print(tokenized_text.tokens)\n",
        "print(tokenized_text.ids)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['이', '##순', '##신은', '조선', '중', '##기의', '무신', '##이다', '.']\n",
            "[706, 1288, 7611, 2002, 754, 2606, 13165, 1897, 17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLE2ZQEp5GBq"
      },
      "source": [
        "## BERT 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrihNAMK5EsC",
        "outputId": "a6f61683-339d-4ba7-f154-29960df6633c"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zye2l6fP6igt"
      },
      "source": [
        "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnx2l-RVZOfk"
      },
      "source": [
        "위 과정에서 생성한 tokenizer를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wtm3umi6geG"
      },
      "source": [
        "tokenizer = BertTokenizerFast(\n",
        "    vocab_file='/content/wordPieceTokenizer/my_tokenizer-vocab.txt',\n",
        "    max_len=128,\n",
        "    do_lower_case=False,\n",
        "    )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvjurG9_bWND",
        "outputId": "47e94667-6b3e-439f-e865-dbfa4cd53abd"
      },
      "source": [
        "print(tokenizer.tokenize(\"뷁은 [MASK] 조선 중기의 무신이다.\"))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[UNK]', '[', 'M', '##AS', '##K', ']', '조선', '중', '##기의', '무신', '##이다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpmBH_F5XYxf",
        "outputId": "d31aa32d-3d85-4cdc-97e3-e005fd9b8fba"
      },
      "source": [
        "tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n",
        "print(tokenizer.tokenize(\"이순신은 [MASK] 중기의 무신이다.\"))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH7JzbMv5xBt",
        "outputId": "8fff6e1e-a8f6-48ce-cd56-3419c67b32d9"
      },
      "source": [
        "from transformers import BertConfig, BertForPreTraining\n",
        "\n",
        "config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n",
        "    vocab_size=20000,\n",
        "    # hidden_size=512,\n",
        "    # num_hidden_layers=12,    # layer num\n",
        "    # num_attention_heads=8,    # transformer attention head number\n",
        "    # intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n",
        "    # hidden_act=\"gelu\",\n",
        "    # hidden_dropout_prob=0.1,\n",
        "    # attention_probs_dropout_prob=0.1,\n",
        "    max_position_embeddings=128,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정\n",
        "    # type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)\n",
        "    # pad_token_id=0,\n",
        "    # position_embedding_type=\"absolute\"\n",
        ")\n",
        "\n",
        "model = BertForPreTraining(config=config)\n",
        "model.num_parameters()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101720098"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7SnD0VjUqi"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ka2mjYRuKzo"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from typing import Dict, List, Optional\n",
        "import os\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from filelock import FileLock\n",
        "\n",
        "from transformers.utils import logging\n",
        "\n",
        "logger = logging.get_logger(__name__)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNGbscSSZzLv"
      },
      "source": [
        "BERT의 학습에서 가장 중요한 요소 중 하나는, 다음 문장을 예측하는 것 입니다.   \n",
        "아래 코드를 통해 다음 문장을 예측하기 위한 dataset을 구성합니다.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ekdXKzlt2qO"
      },
      "source": [
        "class TextDatasetForNextSentencePrediction(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        file_path: str,\n",
        "        block_size: int,\n",
        "        overwrite_cache=False,\n",
        "        short_seq_probability=0.1,\n",
        "        nsp_probability=0.5,\n",
        "    ):\n",
        "        # 여기 부분은 학습 데이터를 caching하는 부분입니다 :-)\n",
        "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
        "\n",
        "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
        "        self.short_seq_probability = short_seq_probability\n",
        "        self.nsp_probability = nsp_probability\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            directory,\n",
        "            \"cached_nsp_{}_{}_{}\".format(\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(block_size),\n",
        "                filename,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        lock_path = cached_features_file + \".lock\"\n",
        "\n",
        "        # Input file format:\n",
        "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
        "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
        "        # sentence boundaries for the \"next sentence prediction\" task).\n",
        "        # (2) Blank lines between documents. Document boundaries are needed so\n",
        "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
        "        #\n",
        "        # Example:\n",
        "        # I am very happy.\n",
        "        # Here is the second sentence.\n",
        "        #\n",
        "        # A new document.\n",
        "\n",
        "        with FileLock(lock_path):\n",
        "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"rb\") as handle:\n",
        "                    self.examples = pickle.load(handle)\n",
        "                logger.info(\n",
        "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
        "                )\n",
        "            else:\n",
        "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
        "                # 여기서부터 본격적으로 dataset을 만듭니다.\n",
        "                self.documents = [[]]\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    while True: # 일단 문장을 읽고\n",
        "                        line = f.readline()\n",
        "                        if not line:\n",
        "                            break\n",
        "                        line = line.strip()\n",
        "\n",
        "                        # 이중 띄어쓰기가 발견된다면, 나왔던 문장들을 모아 하나의 문서로 묶어버립니다.\n",
        "                        # 즉, 문단 단위로 데이터를 저장합니다.\n",
        "                        if not line and len(self.documents[-1]) != 0:\n",
        "                            self.documents.append([])\n",
        "                        tokens = tokenizer.tokenize(line)\n",
        "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "                        if tokens:\n",
        "                            self.documents[-1].append(tokens)\n",
        "                # 이제 코퍼스 전체를 읽고, 문서 데이터를 생성했습니다! :-)\n",
        "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
        "                self.examples = []\n",
        "                # 본격적으로 학습을 위한 데이터로 변형시켜볼까요?\n",
        "                for doc_index, document in enumerate(self.documents):\n",
        "                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다.\n",
        "\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"wb\") as handle:\n",
        "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                logger.info(\n",
        "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
        "                )\n",
        "\n",
        "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
        "        \"\"\"Creates examples for a single document.\"\"\"\n",
        "        # 문장의 앞, 뒤에 [CLS], [SEP] token이 부착되기 때문에, 내가 지정한 size에서 2 만큼 빼줍니다.\n",
        "        # 예를 들어 128 token 만큼만 학습 가능한 model을 선언했다면, 학습 데이터로부터는 최대 126 token만 가져오게 됩니다.\n",
        "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
        "\n",
        "        # We *usually* want to fill up the entire sequence since we are padding\n",
        "        # to `block_size` anyways, so short sequences are generally wasted\n",
        "        # computation. However, we *sometimes*\n",
        "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
        "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
        "        # The `target_seq_length` is just a rough target however, whereas\n",
        "        # `block_size` is a hard limit.\n",
        "\n",
        "        # 여기가 재밌는 부분인데요!\n",
        "        # 위에서 설명했듯이, 학습 데이터는 126 token(128-2)을 채워서 만들어지는게 목적입니다.\n",
        "        # 하지만 나중에 BERT를 사용할 때, 126 token 이내의 짧은 문장을 테스트하는 경우도 분명 많을 것입니다 :-)\n",
        "        # 그래서 short_seq_probability 만큼의 데이터에서는 2-126 사이의 random 값으로 학습 데이터를 만들게 됩니다.\n",
        "        target_seq_length = max_num_tokens\n",
        "        if random.random() < self.short_seq_probability:\n",
        "            target_seq_length = random.randint(2, max_num_tokens)\n",
        "\n",
        "        current_chunk = []  # a buffer stored current working segments\n",
        "        current_length = 0\n",
        "        i = 0\n",
        "\n",
        "        # 데이터 구축의 단위는 document 입니다\n",
        "        # 이 때, 무조건 문장_1[SEP]문장_2 이렇게 만들어지는 것이 아니라,\n",
        "        # 126 token을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 만들어질 수 있습니다.\n",
        "        while i < len(document):\n",
        "            segment = document[i]\n",
        "            current_chunk.append(segment)\n",
        "            current_length += len(segment)\n",
        "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
        "                if current_chunk:\n",
        "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
        "                    # (first) sentence.\n",
        "                    a_end = 1\n",
        "                    # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n",
        "                    if len(current_chunk) >= 2:\n",
        "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
        "                    tokens_a = []\n",
        "                    for j in range(a_end):\n",
        "                        tokens_a.extend(current_chunk[j])\n",
        "                    # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n",
        "                    tokens_b = []\n",
        "                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n",
        "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
        "                        is_random_next = True\n",
        "                        target_b_length = target_seq_length - len(tokens_a)\n",
        "\n",
        "                        # This should rarely go for more than one iteration for large\n",
        "                        # corpora. However, just to be careful, we try to make sure that\n",
        "                        # the random document is not the same as the document\n",
        "                        # we're processing.\n",
        "                        for _ in range(10):\n",
        "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
        "                            if random_document_index != doc_index:\n",
        "                                break\n",
        "                        # 여기서 랜덤하게 선택합니다 :-)\n",
        "                        random_document = self.documents[random_document_index]\n",
        "                        random_start = random.randint(0, len(random_document) - 1)\n",
        "                        for j in range(random_start, len(random_document)):\n",
        "                            tokens_b.extend(random_document[j])\n",
        "                            if len(tokens_b) >= target_b_length:\n",
        "                                break\n",
        "                        # We didn't actually use these segments so we \"put them back\" so\n",
        "                        # they don't go to waste.\n",
        "                        num_unused_segments = len(current_chunk) - a_end\n",
        "                        i -= num_unused_segments\n",
        "                    # Actual next\n",
        "                    else:\n",
        "                        is_random_next = False\n",
        "                        for j in range(a_end, len(current_chunk)):\n",
        "                            tokens_b.extend(current_chunk[j])\n",
        "\n",
        "                    # 이제 126 token을 넘는다면 truncation을 해야합니다.\n",
        "                    # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n",
        "                    # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n",
        "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
        "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
        "                        while True:\n",
        "                            total_length = len(tokens_a) + len(tokens_b)\n",
        "                            if total_length <= max_num_tokens:\n",
        "                                break\n",
        "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
        "                            assert len(trunc_tokens) >= 1\n",
        "                            # We want to sometimes truncate from the front and sometimes from the\n",
        "                            # back to add more randomness and avoid biases.\n",
        "                            if random.random() < 0.5:\n",
        "                                del trunc_tokens[0]\n",
        "                            else:\n",
        "                                trunc_tokens.pop()\n",
        "\n",
        "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
        "\n",
        "                    assert len(tokens_a) >= 1\n",
        "                    assert len(tokens_b) >= 1\n",
        "\n",
        "                    # add special tokens\n",
        "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
        "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
        "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
        "                    \n",
        "                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
        "                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
        "                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
        "                    example = {\n",
        "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
        "                    }\n",
        "\n",
        "                    self.examples.append(example)\n",
        "\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "\n",
        "            i += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.examples[i]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvlBlEmF52ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d3a19f-32e6-40be-d2da-5b0d7b803952"
      },
      "source": [
        "dataset = TextDatasetForNextSentencePrediction(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/my_data/wiki_20190620_small.txt',\n",
        "    block_size=128,\n",
        "    overwrite_cache=False,\n",
        "    short_seq_probability=0.1,\n",
        "    nsp_probability=0.5,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (138 > 128). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt52m1cUbCIl"
      },
      "source": [
        "이렇게 만들어진 학습 데이터를 확인해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZGsKlvDQBBI",
        "outputId": "02b252c6-3da1-4977-996d-0458afb2f502"
      },
      "source": [
        "for example in dataset.examples[0:1]:\n",
        "    print(example)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([    2,  4356,   640,     6,  5499,     6,  5506, 11107,  2492,  2430,\n",
            "         2782,  1970,  5381,  3116,  1942,  2409,    17,     3,  1415,  1435,\n",
            "          931, 16493, 12288,  1017,  3669,    17,  6531,  8936,  1007,  2679,\n",
            "         1908,    17,   175,   985,  4022,  1016,  8598,   728,  1265,    94,\n",
            "         7744,    94, 10416,  1066, 18360,  3495, 18881,    17,  6438,  1970,\n",
            "         4022,   280,  3360,   658,  1397,  2107,  1935, 17659,    94,   440,\n",
            "         1139,  2139,     1,  2025,  4088, 17974,    17,  2064,   497,  2737,\n",
            "            6, 17659, 12980,     6,   380,  7720,    17,  4189,  6531,   750,\n",
            "          543,  1107,  2797,  4863,  5156,  4773,   175, 11847, 15564,   655,\n",
            "         2788,  9397,  1947,  2372,  2896,  2056,    15,  9873,  6531,   750,\n",
            "          762,  1135,  6462,  5156,  2825,  3952,  6531,   750,   762,  2094,\n",
            "         6208,  1901,    17,  2831,  4532,   728, 16250, 18211,  2664, 17622,\n",
            "        13576,  2495,    15,  3953,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1]), 'next_sentence_label': tensor(0)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdwJsZDdWpni"
      },
      "source": [
        "[MASK]를 부착하는 data collator의 역할은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpeY7jjMWsTW",
        "outputId": "9d838e9c-e90a-477e-f1e9-2278c93179d2"
      },
      "source": [
        "print(data_collator(dataset.examples))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    2,  4356,   640,  ...,    15,  3953,     3],\n",
            "        [    2,  6531,  7895,  ...,  7052,  6127,     3],\n",
            "        [    2,  1985,   750,  ...,   175,  4176,     3],\n",
            "        ...,\n",
            "        [    2,   757,  3611,  ...,  5194,     4,     3],\n",
            "        [    2,  1917,  2025,  ...,  2032,    17,     3],\n",
            "        [    2,     4, 10887,  ..., 12018,    17,     3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1]]), 'next_sentence_label': tensor([0, 0, 0,  ..., 1, 1, 1]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
            "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
            "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
            "        ...,\n",
            "        [ -100,  -100,  -100,  ...,  -100,  3402,  -100],\n",
            "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
            "        [ -100, 12581,  -100,  ...,  -100,  -100,  -100]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMMUbKr_WvhG",
        "outputId": "966da156-8234-4880-a3af-c0b8bc5253c1"
      },
      "source": [
        "print(data_collator(dataset.examples)['input_ids'][0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([    2,     4,   640,     6,  5499,     6,  5506, 11107,  2492,  2430,\n",
            "         2782,  1970,  5381,  3116,  1942,  2409,    17,     3,  1415,  1435,\n",
            "          931, 16493,     4,  1017,  3669,    17,  6531,  8936,  1007,  2679,\n",
            "         1908,    17,   175,   985,  4022,  1016,  8598,   728,  1265,    94,\n",
            "         7744,    94, 10416,  1066, 18360,     4,     4,    17,     4,  1970,\n",
            "         4022,   280,     4,   658,  1397,  2107,  1935, 17659,    94,   440,\n",
            "         1139,  2139,     1,  2025,  4088, 17974,     4,  2064,   497,     4,\n",
            "            6, 17659, 12980,     6,   380,  7720,    17,  4189,  6531,   750,\n",
            "          543,  1107,  2797,  4863,  5156,  4773,   175, 11847, 15564,     4,\n",
            "         2788,  9397,  1947,  2372,  2896,  2056,    15,     4,  6531,   750,\n",
            "          762,  1135,  6462,  5156,  2825,  3952,  6531,   750,   762,  2094,\n",
            "         6208,  1901,    17,  2831,  4532,     4, 16250, 18211,  2664, 17622,\n",
            "            4,  2495,    15,  3953,     3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWbI_dEJeguW"
      },
      "source": [
        "collator 함수가 실행되면, 입력 문장에 [MASK] 가 부착됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "5lzsg6RIVIIN",
        "outputId": "53f009b7-10b6-4176-8935-d5564d04e1eb"
      },
      "source": [
        "tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] 제임스 얼 [MASK] 지미 \" 카터 주니어는 불암산 출신 미국 39번째 대통령 이다. [SEP] [MASK]티 플레인스 마을에 [MASK] 태어났다. 조지아 공과대학교를 [MASK]하였다. 그 않고 해군에 들어가 전함 · 원자력 · 잠수함의 승무원으로 일하였다. 1953년 미국 [MASK] 대위로 예르의하였고 이후 땅콩 · 면화 등을 [UNK] 많은 돈을 벌었다. 그의 [MASK]명이 \" 땅콩 농부 \" 로 알려졌다. 1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 [MASK] 당선되고 [MASK] 1966년 조지아 주 지 장시간 선거에 낙선하지만 1970년 [MASK] 주 지사를 역임했다. [MASK] 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년 [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEGfg7JL7KWJ"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='model_output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_gpu_train_batch_size=32,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwdMy08j7u-F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "a8473586-3067-4812-ba00-bfce416e6f10"
      },
      "source": [
        "trainer.train() # wiki 전체 데이터로 학습 시, 1 epoch에 9시간 정도 소요됩니다!! "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='30' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 30/750 00:22 < 09:39, 1.24 it/s, Epoch 0.39/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F-dEL8TZ1eo"
      },
      "source": [
        "trainer.save_model('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtuDt--Yb1Bx"
      },
      "source": [
        "BERT 학습이 완료되었습니다! :-)   \n",
        "아래 코드를 실행해서 [MASK] 테스트도 가능합니다.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrKvxFAMVg9I"
      },
      "source": [
        "from transformers import BertForMaskedLM, pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCg6A9rMVsIY"
      },
      "source": [
        "my_model = BertForMaskedLM.from_pretrained('model_output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e1pqEqKWgGT"
      },
      "source": [
        "tokenizer.tokenize('이순신은 [MASK] 중기의 무신이다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVYg7-PmVzvH"
      },
      "source": [
        "nlp_fill = pipeline('fill-mask', top_k=5, model=my_model, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPn9azZoV7aC"
      },
      "source": [
        "nlp_fill('이순신은 [MASK] 중기의 무신이다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kYenPbpZqhP"
      },
      "source": [
        "nlp_fill('[MASK]는 조선 중기의 무신이다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AvZG_qmQKBL"
      },
      "source": [
        "끗!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9JiMJ3gaQUC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}